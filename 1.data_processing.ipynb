{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,glob\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_files = {}\n",
    "for i in glob.glob('data/raw/*txt'):\n",
    "    doc = open(i).read().splitlines()\n",
    "    original_files[i[-5]] = doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26508, 4)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_verse = []\n",
    "corpus_book = []\n",
    "corpus_english = []\n",
    "corpus_latin = []\n",
    "\n",
    "for i in glob.glob('data/raw/*txt'):\n",
    "    doc = open(i).read().splitlines()\n",
    "    for index,sentence in enumerate(doc):\n",
    "        splitted = sentence.split(\"||\") \n",
    "        if len(splitted)==3:\n",
    "            file_name = i[-5]\n",
    "            index = splitted[0].strip()\n",
    "            english = splitted[1].strip()\n",
    "            latin = splitted[2].strip()\n",
    "            if bool(index) and bool(english) and bool(latin):\n",
    "                if len(english)>=17: \n",
    "                    corpus_verse.append(index)\n",
    "                    corpus_book.append(file_name)\n",
    "                    corpus_english.append(english)\n",
    "                    corpus_latin.append(latin)\n",
    "\n",
    "df = pd.DataFrame({\"verse\":corpus_verse,\"book\":corpus_book,\n",
    "\"english\":corpus_english,\"latin\":corpus_latin})\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inferred rules english:\n",
    "#.replace('ð', 'þ')\n",
    "#Uppercase letters don't strike any difference\n",
    "\n",
    "# Inferred rules for latin\n",
    "# Classical Latin did not have a lower-case/upper-case distinction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"english\"] = df.english.str.replace('ð', 'þ').replace('æ', 'ae').replace('ę', 'ae').replace('œ', 'oe').str.lower()\n",
    "df[\"latin\"] = df.latin.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "def remove_punctuations(text):\n",
    "    for punctuation in string.punctuation:\n",
    "        text = text.replace(punctuation, '')\n",
    "    return text\n",
    "    \n",
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check punctuation!\n",
    "# I will remove parenthesis and punctuation\n",
    "df[\"english\"] = df['english'].apply(remove_punctuations)\n",
    "df[\"latin\"] = df['latin'].apply(remove_punctuations)\n",
    "\n",
    "#Remove multiple spaces \n",
    "df[\"latin\"] = df.latin.replace(r'\\s+', ' ', regex=True)\n",
    "df[\"english\"] = df.english.replace(r'\\s+', ' ', regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First look for stopwords, lemmatiser and stemmer for old english. For latin there must be something else"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cltk.lemmatize import OldEnglishDictionaryLemmatizer\n",
    "# Explore lemmas\n",
    "# OldEnglishDictionaryLemmatizer()._load_forms_and_lemmas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cltk.lemmatize import OldEnglishLemmatizationProcess, LatinLemmatizationProcess\n",
    "from cltk.lemmatize.lat import LatinBackoffLemmatizer\n",
    "## Lemmatizing with pipeline\n",
    "from cltk.core.data_types import Process, Pipeline\n",
    "from cltk.tokenizers import MultilingualTokenizationProcess\n",
    "from cltk.languages.utils import get_lang\n",
    "from cltk.nlp import NLP\n",
    "from cltk.stops.ang import STOPS as ANG_STOPS\n",
    "from cltk.stops.lat import STOPS as LAT_STOPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['and',\n",
       " 'on',\n",
       " 'þonne',\n",
       " 'wið',\n",
       " 'to',\n",
       " 'þæt',\n",
       " 'þe',\n",
       " 'ne',\n",
       " 'ic',\n",
       " 'me',\n",
       " 'heo',\n",
       " 'him',\n",
       " 'he',\n",
       " 'swa',\n",
       " 'þis',\n",
       " 'mid',\n",
       " 'þu',\n",
       " 'ofer',\n",
       " 'his',\n",
       " 'þriwa',\n",
       " 'seo',\n",
       " 'hit',\n",
       " 'se',\n",
       " 'þas',\n",
       " 'cweð',\n",
       " 'þæs',\n",
       " 'in',\n",
       " 'sy',\n",
       " 'ða',\n",
       " 'ðy',\n",
       " 'ær',\n",
       " 'ðonne',\n",
       " 'næfre',\n",
       " 'þone',\n",
       " 'ge',\n",
       " 'ðas',\n",
       " 'þære',\n",
       " 'þam',\n",
       " 'is',\n",
       " 'of',\n",
       " 'gif',\n",
       " 'þæm',\n",
       " 'nu',\n",
       " 'under',\n",
       " 'wiþ',\n",
       " 'geond',\n",
       " 'æfter',\n",
       " 'ðis',\n",
       " 'do',\n",
       " 'hwæt',\n",
       " 'her',\n",
       " 'þurh',\n",
       " 'þus',\n",
       " 'lytel',\n",
       " 'æt',\n",
       " 'ðin',\n",
       " 'willian',\n",
       " 'cume',\n",
       " 'þeos',\n",
       " 'þara',\n",
       " 'are',\n",
       " 'cuman',\n",
       " 'com',\n",
       " 'ænig',\n",
       " 'þon',\n",
       " 'for',\n",
       " 'us',\n",
       " 'ac',\n",
       " 'bot',\n",
       " 'ende',\n",
       " 'wæs',\n",
       " 'wǣre',\n",
       " 'wes',\n",
       " 'wǣron',\n",
       " 'wǣren',\n",
       " 'wesað',\n",
       " 'ic',\n",
       " 'wit',\n",
       " 'wē',\n",
       " 'mīn',\n",
       " 'uncer',\n",
       " 'ūser',\n",
       " 'ūre',\n",
       " 'mē',\n",
       " 'unc',\n",
       " 'ūs',\n",
       " 'mec',\n",
       " 'uncit',\n",
       " 'ūsic',\n",
       " 'ðū',\n",
       " 'git',\n",
       " 'gē',\n",
       " 'ðīn',\n",
       " 'incer',\n",
       " 'ēower',\n",
       " 'ēowre',\n",
       " 'ðē',\n",
       " 'inc',\n",
       " 'ēow',\n",
       " 'ðec',\n",
       " 'incit',\n",
       " 'ēowic',\n",
       " 'hē',\n",
       " 'hēo',\n",
       " 'hīe',\n",
       " 'hit',\n",
       " 'hyt',\n",
       " 'hī',\n",
       " 'hȳ',\n",
       " 'hire',\n",
       " 'hira',\n",
       " 'heora',\n",
       " 'hiera',\n",
       " 'heom',\n",
       " 'hine',\n",
       " 'nǣr',\n",
       " 'nǣfre',\n",
       " 'nǣnig',\n",
       " 'nolde',\n",
       " 'noldon',\n",
       " 'be',\n",
       " 'beforan',\n",
       " 'betweox',\n",
       " 'for',\n",
       " 'from',\n",
       " 'fram',\n",
       " 'mid',\n",
       " 'tō',\n",
       " 'geond',\n",
       " 'oð',\n",
       " 'þurh',\n",
       " 'ofer',\n",
       " 'under',\n",
       " 'bēo',\n",
       " 'bist',\n",
       " 'biþ',\n",
       " 'bēoþ',\n",
       " 'bēon',\n",
       " 'ēom',\n",
       " 'sīe',\n",
       " 'eart',\n",
       " 'sī',\n",
       " 'is',\n",
       " 'sēo',\n",
       " 'sindon',\n",
       " 'sint',\n",
       " 'nēom',\n",
       " 'neart',\n",
       " 'nis',\n",
       " 'sīo',\n",
       " 'ðæt',\n",
       " 'tæt',\n",
       " 'ðæs',\n",
       " 'ðǣre',\n",
       " 'ðǣm',\n",
       " 'ðām',\n",
       " 'ðone',\n",
       " 'ðā',\n",
       " 'ðȳ',\n",
       " 'ðē',\n",
       " 'ðon',\n",
       " 'ðāra',\n",
       " 'ðǣra',\n",
       " 'ðes',\n",
       " 'ðēos',\n",
       " 'ðisse',\n",
       " 'ðeosse',\n",
       " 'ðises',\n",
       " 'ðisses',\n",
       " 'ðisum',\n",
       " 'ðissum',\n",
       " 'ðisne',\n",
       " 'ðās',\n",
       " 'ðīs',\n",
       " 'ðȳs',\n",
       " 'ðissa',\n",
       " 'ðeossa',\n",
       " 'ðeosum',\n",
       " 'ðeossum',\n",
       " 'twēgen',\n",
       " 'twā',\n",
       " 'tū',\n",
       " 'twēgra',\n",
       " 'twǣm',\n",
       " 'þrīe',\n",
       " 'þrēo',\n",
       " 'þrēora',\n",
       " 'þrīm',\n",
       " 'endlefan',\n",
       " 'twelf',\n",
       " 'twēntig',\n",
       " 'þrēotīene',\n",
       " 'þrītig',\n",
       " 'fēower',\n",
       " 'fēowertīene',\n",
       " 'fēowertig',\n",
       " 'fīf',\n",
       " 'fīftīene',\n",
       " 'fīftig',\n",
       " 'siex',\n",
       " 'siextīene',\n",
       " 'siextig',\n",
       " 'seofon',\n",
       " 'seofontīene',\n",
       " 'seofontig',\n",
       " 'eahta',\n",
       " 'eahtatīene',\n",
       " 'eahtatig',\n",
       " 'nigon',\n",
       " 'nigontīene',\n",
       " 'nigontig',\n",
       " 'tīen',\n",
       " 'hund',\n",
       " 'gā',\n",
       " 'gǣst',\n",
       " 'gǣð',\n",
       " 'gāð',\n",
       " 'gān',\n",
       " 'gānde',\n",
       " 'gangende',\n",
       " 'gegān',\n",
       " 'ēode',\n",
       " 'ēodest',\n",
       " 'ēodon',\n",
       " 'ēoden']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ANG_STOPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_eng = Pipeline(description=\"A custom Old English pipeline\", \\\n",
    "    processes=[MultilingualTokenizationProcess, OldEnglishLemmatizationProcess], \\\n",
    "    language=get_lang(\"ang\"))\n",
    "\n",
    "nlp_eng = NLP(language='ang', custom_pipeline=pipe_eng, suppress_banner=True)\n",
    "\n",
    "\n",
    "latin_lem = LatinBackoffLemmatizer()\n",
    "\n",
    "# #Latin pipeline not working. Lemmatized text remains the same\n",
    "# pipe_latin = Pipeline(description=\"A custom Old Latin pipeline\", \\\n",
    "#     processes=[MultilingualTokenizationProcess, LatinLemmatizationProcess], \\\n",
    "#     language=get_lang(\"ang\"))\n",
    "\n",
    "# nlp_latin = NLP(language='lat', custom_pipeline=pipe_latin, suppress_banner=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check for lemmatization examples of the rules given by Prof."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {\"æ\":\"ae\",\n",
    "# \"ę\":\"ae\",\n",
    "# \"œ\":\"oe\"\n",
    "# \"michi\" to \"mihi\". \n",
    "# But that still leaves important variation that you don't \n",
    "# want to replace globally: \"e\" should sometimes be \"ae\" \n",
    "# (e.g. \"eternus\" will only be recognized once it is \"aeternus\"), \n",
    "# and \"c\" should sometimes be \"t\" (\"laudacio\")\n",
    "\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'forþon þe drihten on cnosle ł mægþe on rihtwisre is geþeaht þæs hæfenleasan ł wædlan ge forsawon ł ge gescendon forþan þe drihten hiht his is'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[149,\"english\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_eng = []\n",
    "lemmatized_lat = []\n",
    "for (_,verse,book,english,latin) in df.itertuples(name=None):\n",
    "    eng_lemma_no_stopw = [x for x in nlp_eng(english).lemmata if x not in ANG_STOPS]\n",
    "    lemmatized_eng.append(\" \".join(eng_lemma_no_stopw))\n",
    "    #Latin\n",
    "    lemma_list = [lemma for (word,lemma) in latin_lem.lemmatize(latin.split()) if lemma not in LAT_STOPS]\n",
    "    lemmatized_lat.append(\" \".join(lemma_list))\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"english_lemma\"] = lemmatized_eng\n",
    "df[\"latin_lemma\"] = lemmatized_lat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ab',\n",
       " 'ac',\n",
       " 'ad',\n",
       " 'adhic',\n",
       " 'aliqui',\n",
       " 'aliquis',\n",
       " 'an',\n",
       " 'ante',\n",
       " 'apud',\n",
       " 'at',\n",
       " 'atque',\n",
       " 'aut',\n",
       " 'autem',\n",
       " 'cum',\n",
       " 'cur',\n",
       " 'de',\n",
       " 'deinde',\n",
       " 'dum',\n",
       " 'ego',\n",
       " 'enim',\n",
       " 'ergo',\n",
       " 'es',\n",
       " 'est',\n",
       " 'et',\n",
       " 'etiam',\n",
       " 'etsi',\n",
       " 'ex',\n",
       " 'fio',\n",
       " 'haud',\n",
       " 'hic',\n",
       " 'iam',\n",
       " 'idem',\n",
       " 'igitur',\n",
       " 'ille',\n",
       " 'in',\n",
       " 'infra',\n",
       " 'inter',\n",
       " 'interim',\n",
       " 'ipse',\n",
       " 'is',\n",
       " 'ita',\n",
       " 'magis',\n",
       " 'modo',\n",
       " 'mox',\n",
       " 'nam',\n",
       " 'ne',\n",
       " 'nec',\n",
       " 'necque',\n",
       " 'neque',\n",
       " 'nisi',\n",
       " 'non',\n",
       " 'nos',\n",
       " 'o',\n",
       " 'ob',\n",
       " 'per',\n",
       " 'possum',\n",
       " 'post',\n",
       " 'pro',\n",
       " 'quae',\n",
       " 'quam',\n",
       " 'quare',\n",
       " 'qui',\n",
       " 'quia',\n",
       " 'quicumque',\n",
       " 'quidem',\n",
       " 'quilibet',\n",
       " 'quis',\n",
       " 'quisnam',\n",
       " 'quisquam',\n",
       " 'quisque',\n",
       " 'quisquis',\n",
       " 'quo',\n",
       " 'quoniam',\n",
       " 'sed',\n",
       " 'si',\n",
       " 'sic',\n",
       " 'sive',\n",
       " 'sub',\n",
       " 'sui',\n",
       " 'sum',\n",
       " 'super',\n",
       " 'suus',\n",
       " 'tam',\n",
       " 'tamen',\n",
       " 'trans',\n",
       " 'tu',\n",
       " 'tum',\n",
       " 'ubi',\n",
       " 'uel',\n",
       " 'uero',\n",
       " 'unus',\n",
       " 'ut']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LAT_STOPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22920\n",
      "in principio istius psalmi loquitur deus deinde numerosis diuisionibus diuiditur\n",
      "Original: in principio istius psalmi loquitur deus deinde numerosis diuisionibus diuiditur\n",
      "Lemma: principium iste psalmus loquor deus numerosus diuisio divido\n"
     ]
    }
   ],
   "source": [
    "#Validating examples\n",
    "for index, row in df.iterrows():\n",
    "    if row[\"english\"] != row[\"english_lemma\"] and \" deinde \" in row[\"latin\"]:\n",
    "        print(index)\n",
    "        print(row[\"latin\"])\n",
    "        print(\"Original:\",row[\"latin\"])\n",
    "        print(\"Lemma:\",row[\"latin_lemma\"])\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(\"preprocessed.xlsx\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('cmptc-proj')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15 (main, Nov 24 2022, 14:31:59) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a10a147f3fb2d9351807707eb2e679a95fdb0cf9aa4379fe03f45b7ce9bd3344"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
